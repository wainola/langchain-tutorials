{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabffae0",
   "metadata": {},
   "source": [
    "## Example with LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c17f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.document_loaders.json_loader.JSONLoader'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "print(JSONLoader)\n",
    "questions = None\n",
    "\n",
    "with open('./data/questions.json', 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = file.read()\n",
    "    questions = data\n",
    "\n",
    "# print(questions)\n",
    "\n",
    "# this is using json loader\n",
    "loader = JSONLoader(file_path=\"./data/questions.json\", jq_schema=\"[]\", text_content=False)\n",
    "data_loaded = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e36182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v2 (pre #1508)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. First we need to identify what is the data structure of json file\n",
      "2. Next, We will have to decide whether it is necessary to use any library or not to read and parse the json file data\n",
      "3. After that , we can create a function to accept the json file as input and return the result\n",
      "4. Finally, We need to call this function in our main program and pass the path of json file as an argument\n",
      "5. And we will get the result of type value from the function\n",
      "\n",
      "It depends on what you want to do with the data in the json file, but common libraries used for reading and parsing json files are 'json' and 'huggingface/transformers'.\n",
      "\n",
      "for example using python's built-in library 'json' :\n",
      "```python\n",
      "import json\n",
      "\n",
      "def parse_json(file_path):\n",
      "    with open(file_path, \"r\") as file:\n",
      "        data = json.load(file)\n",
      "    return data\n",
      "```\n",
      "And using Hugging Face's Transformers library:\n",
      "```python\n",
      "from transformers import pipeline\n",
      "\n",
      "def parse_json(file_path):\n",
      "    model = pipeline(\"tokenizer\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12141.73 ms\n",
      "llama_print_timings:      sample time =   176.38 ms /   256 runs   (    0.69 ms per token)\n",
      "llama_print_timings: prompt eval time = 13848.96 ms /    24 tokens (  577.04 ms per token)\n",
      "llama_print_timings:        eval time = 28453.93 ms /   255 runs   (  111.58 ms per token)\n",
      "llama_print_timings:       total time = 46456.63 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. First we need to identify what is the data structure of json file\\n2. Next, We will have to decide whether it is necessary to use any library or not to read and parse the json file data\\n3. After that , we can create a function to accept the json file as input and return the result\\n4. Finally, We need to call this function in our main program and pass the path of json file as an argument\\n5. And we will get the result of type value from the function\\n\\nIt depends on what you want to do with the data in the json file, but common libraries used for reading and parsing json files are \\'json\\' and \\'huggingface/transformers\\'.\\n\\nfor example using python\\'s built-in library \\'json\\' :\\n```python\\nimport json\\n\\ndef parse_json(file_path):\\n    with open(file_path, \"r\") as file:\\n        data = json.load(file)\\n    return data\\n```\\nAnd using Hugging Face\\'s Transformers library:\\n```python\\nfrom transformers import pipeline\\n\\ndef parse_json(file_path):\\n    model = pipeline(\"tokenizer\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./models/ggml-vic13b-q5_1.bin\",\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"how many type values accept a json file?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cae9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
