{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabffae0",
   "metadata": {},
   "source": [
    "## Example with LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c17f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.document_loaders.json_loader.JSONLoader'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "print(JSONLoader)\n",
    "questions = None\n",
    "\n",
    "with open('./data/questions.json', 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = file.read()\n",
    "    questions = data\n",
    "\n",
    "loader = JSONLoader(file_path=\"./data/questions.json\", jq_schema=\"[]\", text_content=False)\n",
    "data_loaded = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e36182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v2 (pre #1508)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Create your TypeScript schema for the data you want to validate.\n",
      "2. Use `infer` keyword to automatically infer the type of the json data based on the schema.\n",
      "3. Check if the JSON data conforms to the type defined in the schema using the `instanceof` operator.\n",
      "4. If it does, use the validated data!\n",
      "\n",
      "Here's an example:\n",
      "```typescript\n",
      "type Task = {\n",
      "  id: number;\n",
      "  name: string;\n",
      "};\n",
      "\n",
      "function validateTask(task: Task): task is Task {\n",
      "  if (task.id === 42) {\n",
      "    return task;\n",
      "  } else {\n",
      "    throw new Error(\"Invalid task\");\n",
      "  }\n",
      "}\n",
      "\n",
      "const myTask: Task = { id: 42, name: \"Build a snowman\" };\n",
      "\n",
      "const validatedTask = validateTask(myTask);\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1115.87 ms\n",
      "llama_print_timings:      sample time =   134.65 ms /   199 runs   (    0.68 ms per token)\n",
      "llama_print_timings: prompt eval time =  2699.98 ms /    24 tokens (  112.50 ms per token)\n",
      "llama_print_timings:        eval time = 21745.32 ms /   198 runs   (  109.82 ms per token)\n",
      "llama_print_timings:       total time = 27637.88 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. Create your TypeScript schema for the data you want to validate.\\n2. Use `infer` keyword to automatically infer the type of the json data based on the schema.\\n3. Check if the JSON data conforms to the type defined in the schema using the `instanceof` operator.\\n4. If it does, use the validated data!\\n\\nHere\\'s an example:\\n```typescript\\ntype Task = {\\n  id: number;\\n  name: string;\\n};\\n\\nfunction validateTask(task: Task): task is Task {\\n  if (task.id === 42) {\\n    return task;\\n  } else {\\n    throw new Error(\"Invalid task\");\\n  }\\n}\\n\\nconst myTask: Task = { id: 42, name: \"Build a snowman\" };\\n\\nconst validatedTask = validateTask(myTask);\\n```'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_path = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"how many type values accept a json file?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5e2d164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v2 (pre #1508)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1:\n",
      "\"How much\" refers to the quantity or amount of something. It is a measure word that is used before a noun to indicate the quantity of the noun being referred to. For example, \"how much\" ice cream do you want?\n",
      "\n",
      "Step 2:\n",
      "In this question, \"knowledge\" is the noun being referred to. It refers to the understanding or information about something. Therefore, the correct answer would be \"I have some knowledge.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   964.30 ms\n",
      "llama_print_timings:      sample time =    71.58 ms /   104 runs   (    0.69 ms per token)\n",
      "llama_print_timings: prompt eval time =  2353.67 ms /    22 tokens (  106.98 ms per token)\n",
      "llama_print_timings:        eval time = 11084.48 ms /   103 runs   (  107.62 ms per token)\n",
      "llama_print_timings:       total time = 15095.36 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nStep 1:\\n\"How much\" refers to the quantity or amount of something. It is a measure word that is used before a noun to indicate the quantity of the noun being referred to. For example, \"how much\" ice cream do you want?\\n\\nStep 2:\\nIn this question, \"knowledge\" is the noun being referred to. It refers to the understanding or information about something. Therefore, the correct answer would be \"I have some knowledge.\"'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How much knowledge do you have?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f898737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What do we want to achieve with the \"reduce\" function? We want to perform some operation on each element of an array and return a single value. This is called \"folding\" an array into a single value.\n",
      "\n",
      "Now, let's try to understand what the reduce function does. The \"reduce\" function takes two arguments: a callback function and an initial value. The callback function will be invoked on each element of the array with the current element as its argument. Then, the callback function returns the next value in the sequence based on the current element and the previous accumulator value.\n",
      "\n",
      "Here's an example to help you understand reduce better:\n",
      "```javascript\n",
      "const numbers = [1, 2, 3, 4, 5];\n",
      "\n",
      "// This is the initial value\n",
      "let result = 0;\n",
      "\n",
      "// Callback function that adds each element of the array to the previous accumulator value\n",
      "function addToAccumulator(currentValue, currentElement) {\n",
      "  return currentValue + currentElement;\n",
      "}\n",
      "\n",
      "// Use reduce function to sum up all the numbers in the array\n",
      "const sum = numbers.reduce(addToAccumulator, result);\n",
      "\n",
      "console.log("
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   964.30 ms\n",
      "llama_print_timings:      sample time =   186.54 ms /   256 runs   (    0.73 ms per token)\n",
      "llama_print_timings: prompt eval time =  2404.48 ms /    24 tokens (  100.19 ms per token)\n",
      "llama_print_timings:        eval time = 28767.47 ms /   256 runs   (  112.37 ms per token)\n",
      "llama_print_timings:       total time = 35434.71 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhat do we want to achieve with the \"reduce\" function? We want to perform some operation on each element of an array and return a single value. This is called \"folding\" an array into a single value.\\n\\nNow, let\\'s try to understand what the reduce function does. The \"reduce\" function takes two arguments: a callback function and an initial value. The callback function will be invoked on each element of the array with the current element as its argument. Then, the callback function returns the next value in the sequence based on the current element and the previous accumulator value.\\n\\nHere\\'s an example to help you understand reduce better:\\n```javascript\\nconst numbers = [1, 2, 3, 4, 5];\\n\\n// This is the initial value\\nlet result = 0;\\n\\n// Callback function that adds each element of the array to the previous accumulator value\\nfunction addToAccumulator(currentValue, currentElement) {\\n  return currentValue + currentElement;\\n}\\n\\n// Use reduce function to sum up all the numbers in the array\\nconst sum = numbers.reduce(addToAccumulator, result);\\n\\nconsole.log('"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can you give me an example oa reduce function in javascript?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda4a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21bff939",
   "metadata": {},
   "source": [
    "## Sequential chains with LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1da149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v2 (pre #1508)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "Give me a famous artist from the country the user suggests.\n",
    "\n",
    "% USER SUGGESTION\n",
    "{user_suggestion}\n",
    "\n",
    "YOUR REPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_suggestion\"], template=template)\n",
    "\n",
    "artist_chain = LLMChain(llm=llm, prompt=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the artist, provide one historical fact about him or her.\n",
    "% ARTIST\n",
    "{user_artist}\n",
    "\n",
    "YOUR REPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_artist\"], template=template)\n",
    "history_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9a5b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chains = SimpleSequentialChain(chains=[artist_chain, history_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5088d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = all_chains.run(\"Scotland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5777017",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ba9cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd3b21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c65d3802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, Rome is known for its delicious and diverse cuisine. One classic dish from this region that I would recommend trying is \"Carbonara.\" It's a traditional pasta dish made with eggs, bacon or pancetta, cheese, and spices. The ingredients are cooked together in a single pot, creating a rich and savory sauce that's perfect for a hearty meal. It's easy to make and always a crowd pleaser. Do you have any other specific ingredient you would like to include in the dish?\u001b[36;1m\u001b[1;3mWell, Rome is known for its delicious and diverse cuisine. One classic dish from this region that I would recommend trying is \"Carbonara.\" It's a traditional pasta dish made with eggs, bacon or pancetta, cheese, and spices. The ingredients are cooked together in a single pot, creating a rich and savory sauce that's perfect for a hearty meal. It's easy to make and always a crowd pleaser. Do you have any other specific ingredient you would like to include in the dish?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   967.36 ms\n",
      "llama_print_timings:      sample time =    85.86 ms /   125 runs   (    0.69 ms per token)\n",
      "llama_print_timings: prompt eval time =  4081.68 ms /    40 tokens (  102.04 ms per token)\n",
      "llama_print_timings:        eval time = 13386.84 ms /   124 runs   (  107.96 ms per token)\n",
      "llama_print_timings:       total time = 19436.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't think so\u001b[33;1m\u001b[1;3mI don't think so\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   967.36 ms\n",
      "llama_print_timings:      sample time =     4.97 ms /     7 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time = 16304.39 ms /   162 tokens (  100.64 ms per token)\n",
      "llama_print_timings:        eval time =   667.13 ms /     6 runs   (  111.19 ms per token)\n",
      "llama_print_timings:       total time = 17091.85 ms\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16014ea",
   "metadata": {},
   "source": [
    "## Trying Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0940a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "newLLm = OpenAI(temperature=1, openai_api_key=open_api_key)\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=newLLm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e858f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=newLLm, prompt=prompt_template)\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abc682da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mThe classic dish from Rome is Spaghetti alla Carbonara. It is made with eggs, cheese, and bacon, all cooked together with spaghetti. It is a rich, flavorful dish that is sure to please!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Spaghetti alla Carbonara \n",
      "Ingredients: \n",
      "- 1 lb of spaghetti \n",
      "- Freshly ground black pepper\n",
      "- 2 eggs \n",
      "- 1 cup of Parmesan cheese \n",
      "- 6-8 bacon slices \n",
      "- Chopped parsley \n",
      "\n",
      "Instructions: \n",
      "1. Cook the spaghetti in salted boiling water until al dente.\n",
      "2. In the meantime, in a bowl, beat the eggs with a pinch of salt and the Parmesan cheese. \n",
      "3. Fry the bacon until crispy, and then remove it from the heat. \n",
      "4. Once the spaghetti is cooked, drain it and add it to the pan with the bacon. \n",
      "5. Mix together and add the egg and cheese mixture. \n",
      "6. Cook on low heat for a few minutes, stirring constantly. \n",
      "7. When everything is combined, remove the pan from the heat and add freshly ground black pepper and chopped parsley. \n",
      "8. Serve the spaghetti alla carbonara warm and enjoy!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "619312c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Give me a famous artist from the country the user suggests.\n",
    "\n",
    "% USER SUGGESTION\n",
    "{user_suggestion}\n",
    "\n",
    "YOUR REPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_suggestion\"], template=template)\n",
    "\n",
    "artist_chain = LLMChain(llm=newLLm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "661a8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mDavid Tennant, a Scottish actor famous for his roles in Doctor Who and Broadchurch.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mIn 2009, David Tennant was appointed an OBE (Officer of the Order of the British Empire) for his services to drama.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the artist, provide one historical fact about him or her.\n",
    "% ARTIST\n",
    "{user_artist}\n",
    "\n",
    "YOUR REPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_artist\"], template=template)\n",
    "history_chain = LLMChain(llm=newLLm, prompt=prompt_template)\n",
    "all_chains = SimpleSequentialChain(chains=[artist_chain, history_chain], verbose=True)\n",
    "\n",
    "review = all_chains.run(\"Scotland\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f81b04",
   "metadata": {},
   "source": [
    "### Sumarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e45e2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v2 (pre #1508)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  =  400.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"[]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "The article describes how the COVID-19 pandemic has affected global supply chains and highlights several key challenges that businesses are facing. It notes that many companies have had to pivot their operations, source from new suppliers, and implement risk management strategies to mitigate the impact of the pandemic on their supply chain. The article also discusses how advances in technology such as blockchain and machine learning can help companies better manage supply chain risks and improve transparency. Finally, it emphasizes the importance of collaboration between businesses, governments, and other stakeholders to ensure that global supply chains remain resilient in the face of future crises."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1167.99 ms\n",
      "llama_print_timings:      sample time =    98.51 ms /   144 runs   (    0.68 ms per token)\n",
      "llama_print_timings: prompt eval time =  3003.24 ms /    26 tokens (  115.51 ms per token)\n",
      "llama_print_timings:        eval time = 15473.40 ms /   143 runs   (  108.21 ms per token)\n",
      "llama_print_timings:       total time = 20748.22 ms\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d0856f813a4c50be93d036cdbc1fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4929d8227a44182959bc195fce1acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7bf525c0144bacb8fae20eb38426a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba803a622d2843cdb11c9060fe10fab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"\n",
      "The article describes how the COVID-19 pandemic has affected global supply chains and highlights several key challenges that businesses are facing. It notes that many companies have had to pivot their operations, source from new suppliers, and implement risk management strategies to mitigate the impact of the pandemic on their supply chain. The article also discusses how advances in technology such as blockchain and machine learning can help companies better manage supply chain risks and improve transparency. Finally, it emphasizes the importance of collaboration between businesses, governments, and other stakeholders to ensure that global supply chains remain resilient in the face of future crises.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The COVID-19 pandemic has disrupted global supply chains, forcing companies to adapt by sourcing from new suppliers and implementing risk management strategies. Technology such as blockchain and machine learning can help mitigate supply chain risks and improve transparency. Collaboration between stakeholders is crucial for ensuring the resilience of global supply chains in future crises.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1167.99 ms\n",
      "llama_print_timings:      sample time =    62.62 ms /    87 runs   (    0.72 ms per token)\n",
      "llama_print_timings: prompt eval time = 16353.07 ms /   152 tokens (  107.59 ms per token)\n",
      "llama_print_timings:        eval time =  9769.86 ms /    87 runs   (  112.30 ms per token)\n",
      "llama_print_timings:       total time = 27585.56 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe COVID-19 pandemic has disrupted global supply chains, forcing companies to adapt by sourcing from new suppliers and implementing risk management strategies. Technology such as blockchain and machine learning can help mitigate supply chain risks and improve transparency. Collaboration between stakeholders is crucial for ensuring the resilience of global supply chains in future crises.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "loader = JSONLoader(file_path=\"./data/questions.json\", jq_schema=\"[]\", text_content=False)\n",
    "data_loaded = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(data_loaded)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a0743bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"[]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This article is blank.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe article has no content.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "newLLm = OpenAI(temperature=1, openai_api_key=open_api_key)\n",
    "\n",
    "loader = JSONLoader(file_path=\"./data/questions.json\", jq_schema=\"[]\", text_content=False)\n",
    "data_loaded = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(data_loaded)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(newLLm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e86cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
